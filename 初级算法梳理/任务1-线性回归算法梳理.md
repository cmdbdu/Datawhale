# 机器学习的一些概念

### 有监督

分类算法和回归算法；
一组训练数据集合，通过输入一个对象和一个输出对象（期望的输出值，也叫做监督信号）；
由新产生的数据，来作为新的输入对象

### 无监督

聚类和降维算法；
根据样本特点，对样本进行识别分类；

### 泛化能力

是指在机器学习中，算法对新数据的适应能力；学习的目的是学到隐含在数据背后的规律，
对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。

### 过拟合欠拟合（方差和偏差以及各自解决办法）

过拟合指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。
通常采用增大数据量和测试样本集的方法对分类器性能进行评价。

欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。

### 交叉验证 

在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。

# 线性回归的原理

线性回归是利用数理统计中回归分析，来统计两种或者两种以上的变量直接相互依赖的关系；

# 线性回归损失函数

是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。
在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。

# 代价函数

是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。
在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。


# 目标函数

目标可以表示成设计变量的数学函数 ，这种函数称为目标函数。

# 优化方法

算法优化是指对算法的有关性能进行优化，如时间复杂度、空间复杂度、正确性、健壮性。
由于算法应用情景变化很大，算法优化可以使算法具有更好泛化能力。
常见的优化方法
>1.随机搜索
是利用随机数求极小点而求得函数近似的最优解的方法。
>2.梯度下降法
梯度下降法是一个最优化算法，通常也称为最速下降法。最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。
最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。
>3.模拟退火法
>4.遗传算法

### 梯度下降法

梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。

### 牛顿法

又称为牛顿-拉弗森方法,它是一种在实数域和复数域上近似求解方程的方法。方法使用函数f(x)的泰勒级数的前面几项来寻找方程f(y)=0的根。

### 拟牛顿法

拟牛顿法和最速下降法(Steepest Descent Methods)一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。

# 线性回归的评估指标
> 残差估计
>> 总体思想是计算实际值与预测值间的差值简称残差。从而实现对回归模型的评估，一般可以画出残差图，进行分析评估、估计模型的异常值、同时还可以检查模型是否是线性的、以及误差是否随机分布。

> 均方误差
>>均方误差是线性模型拟合过程中，最小化误差平方和(SSE)代价函数的平均值。MSE可以用于不同模型的比较，或是通过网格搜索进行参数调优，以及交叉验证等。

> 决定系数
>> 可以看做是MSE的标准化版本，用于更好地解释模型的性能。换句话说，决定系数是模型捕获相应反差的分数。

# sklearn 参数详解

