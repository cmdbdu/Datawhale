# 1.信息论基础
> 熵
>> 一条信息的信息量大小和他的不确定性有直接关系；弄清楚这件不清楚的信息，所需要的信息就是熵；用bits来衡量；
>> 也可以说熵越大，信息不确定性就越大；
>> 公式： $$H[x] = -\sum p(x)log_2p(x)$$

> 联合熵

> 条件熵

> 信息增益
>> 决策树会选择最大化信息增益来对节点进行划分
>> 信息增益计算
>>  $$Info(D) = -\sum_{i=1}^m p_i log_2 (p_i)$$
>>  $$Info_A(D) = \sum_{j=1}^v \frac{|D_j|}{|D|} \times Info(D_j)$$
>>  $$Gain(A) = Info(D) - Info_A(D)$$

> 基尼不纯度

# 决策树的不同分类算法 原理及应用场景
> ID3
>> 选择信息增益最大的节点做根节点
>> 对于连续型数据，需要先切分成离散数据后计算信息增益

> C4.5
>> 是对ID三算法的改进:增益率
>> 数学公式：$$ SplitInfo_a(D) = - \sum_{i=1}^v \frac{|D_j|}{|D|}	\times log_2(\frac{|D_j|}{|D|})$$
>>           $$ GrianRate(A) = \frac {Grian(A)}{SplitInfo_A(D)} $$

> CART分类树
>> CART分类树是生成递归构建二叉决策树的过程；
>> CART 用基尼（Gini） 系数最小化准则来进行特征选择。生成二叉树；
>> Gini系数计算：
>>> $$ Gini(D) = 1 - \sum_{i=1}^m p_i^2 $$
>>> $$ Gini_A(D) = \frac {|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2) $$
>>> $$ \nabla Gini(A) = Gini(D) -Gini_A(D)$$

# 回归数原理

# 决策树防止过拟合手段

# 模型评估

# sklearn参数详解

# python绘制决策树